{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPazfJRLyAKn4TW8wAeP04o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ab5165e09c304699a915bb472fe06221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3bb1a6436074e43bf8e4794a4467e06",
              "IPY_MODEL_b86e49644b4e4a83ba9888ae7e1167c5",
              "IPY_MODEL_b77da89520314dacb367a987cb8e787c"
            ],
            "layout": "IPY_MODEL_0b01502a56124680b9d87b48fdb6be5d"
          }
        },
        "d3bb1a6436074e43bf8e4794a4467e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd20a93eea124ffa966ac101e18ac190",
            "placeholder": "​",
            "style": "IPY_MODEL_f48fef06e5244fef98a19728d39efd54",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "b86e49644b4e4a83ba9888ae7e1167c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f62e70cee34f4517830d42ec95f9a556",
            "max": 443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78a42ae63ab048aa9edb9747d4a46cd6",
            "value": 443
          }
        },
        "b77da89520314dacb367a987cb8e787c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff204ef009a540429d761ab9e61bb54e",
            "placeholder": "​",
            "style": "IPY_MODEL_b4ce836844524f08a40f6dadee76466a",
            "value": " 443/443 [00:00&lt;00:00, 32.2kB/s]"
          }
        },
        "0b01502a56124680b9d87b48fdb6be5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd20a93eea124ffa966ac101e18ac190": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f48fef06e5244fef98a19728d39efd54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f62e70cee34f4517830d42ec95f9a556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78a42ae63ab048aa9edb9747d4a46cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff204ef009a540429d761ab9e61bb54e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4ce836844524f08a40f6dadee76466a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec13f8d45e0946ceb3659fcd1d2d2a66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc7d00b932614ebda46a017b7651888f",
              "IPY_MODEL_0a55d291544f4187b2abc000f174be8f",
              "IPY_MODEL_f5aac1392a34481d8329df8e9bd19f76"
            ],
            "layout": "IPY_MODEL_eb2e480eb21146ac96291d75b20511fd"
          }
        },
        "cc7d00b932614ebda46a017b7651888f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a412920aea434017b209e80680009bf4",
            "placeholder": "​",
            "style": "IPY_MODEL_4dabb12609964d30b2b96f4605b54414",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "0a55d291544f4187b2abc000f174be8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_340bd82069134258a637ae0b5b2d0b5f",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4dc5b79b2bf54f268486e4250d442d84",
            "value": 5069051
          }
        },
        "f5aac1392a34481d8329df8e9bd19f76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6ee5c98fb5d40e0904302037535e44c",
            "placeholder": "​",
            "style": "IPY_MODEL_d504769e561340049cd775a3af4d2667",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 34.1MB/s]"
          }
        },
        "eb2e480eb21146ac96291d75b20511fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a412920aea434017b209e80680009bf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dabb12609964d30b2b96f4605b54414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "340bd82069134258a637ae0b5b2d0b5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dc5b79b2bf54f268486e4250d442d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6ee5c98fb5d40e0904302037535e44c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d504769e561340049cd775a3af4d2667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6a61371281b49bf80c710ca7b803c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c571842e215548d5bfd0a8e2f2776507",
              "IPY_MODEL_bfdd51c8ec24491cae181baadef9bbd9",
              "IPY_MODEL_cac2eba467d7441d89c5cf0044a46211"
            ],
            "layout": "IPY_MODEL_3c080f39c5fa4d7db40646a9aaa10999"
          }
        },
        "c571842e215548d5bfd0a8e2f2776507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_756c968ec1d144b591ea1b5511250908",
            "placeholder": "​",
            "style": "IPY_MODEL_5e4a0d1ee96840108b59e6cad9d82466",
            "value": "tokenizer.json: 100%"
          }
        },
        "bfdd51c8ec24491cae181baadef9bbd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_681db4bd52d1489aabe8abbfa7681d50",
            "max": 17098107,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c9a98297e3047c1be1701c4fd910623",
            "value": 17098107
          }
        },
        "cac2eba467d7441d89c5cf0044a46211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03de0872343040499353966b411cdbb5",
            "placeholder": "​",
            "style": "IPY_MODEL_430fafbc4e0f4d18b056aea569447bb1",
            "value": " 17.1M/17.1M [00:00&lt;00:00, 44.6MB/s]"
          }
        },
        "3c080f39c5fa4d7db40646a9aaa10999": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "756c968ec1d144b591ea1b5511250908": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e4a0d1ee96840108b59e6cad9d82466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "681db4bd52d1489aabe8abbfa7681d50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c9a98297e3047c1be1701c4fd910623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03de0872343040499353966b411cdbb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "430fafbc4e0f4d18b056aea569447bb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8399c2d93c44c50a50c844eb81f9abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0c5e6adaa854492bdf31db59fa20345",
              "IPY_MODEL_33471720cf0c41809dbabe045a7f94a9",
              "IPY_MODEL_ed57f1354fb44fa7b9cb2a432058944b"
            ],
            "layout": "IPY_MODEL_3d9ba94e0dda4285b39277a94a5f3f86"
          }
        },
        "f0c5e6adaa854492bdf31db59fa20345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21da9f75ff9a4c23a0fe144d494c9f44",
            "placeholder": "​",
            "style": "IPY_MODEL_a47d3c18d062486f85af63f339ed8f89",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "33471720cf0c41809dbabe045a7f94a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fb8c060f2ea484fb9d323419c709bfd",
            "max": 279,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_acea3d1613234cef9759dcfd7a5f3372",
            "value": 279
          }
        },
        "ed57f1354fb44fa7b9cb2a432058944b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a05d12af715748778883655cab21f63f",
            "placeholder": "​",
            "style": "IPY_MODEL_7fa6122586414e23af451a639ed75a8b",
            "value": " 279/279 [00:00&lt;00:00, 14.8kB/s]"
          }
        },
        "3d9ba94e0dda4285b39277a94a5f3f86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21da9f75ff9a4c23a0fe144d494c9f44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a47d3c18d062486f85af63f339ed8f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fb8c060f2ea484fb9d323419c709bfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acea3d1613234cef9759dcfd7a5f3372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a05d12af715748778883655cab21f63f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fa6122586414e23af451a639ed75a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fb7e3eb00334944969b35c42ca8fd94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1670cfaffe2440d7b99b7dcb201aa889",
              "IPY_MODEL_941857c8c9ae495ca535a5ba849e3b6d",
              "IPY_MODEL_46307a0fb46944d384cf01e3cb083f64"
            ],
            "layout": "IPY_MODEL_1c3794e9f73146beade36de2e29d035e"
          }
        },
        "1670cfaffe2440d7b99b7dcb201aa889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88ae9052decd4afb9726f525be63d80e",
            "placeholder": "​",
            "style": "IPY_MODEL_0545cca17e2c40eeb4ecb64b33259b45",
            "value": "config.json: 100%"
          }
        },
        "941857c8c9ae495ca535a5ba849e3b6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ccada89eec04f66bf4ec0b575da87ea",
            "max": 801,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef24f6814844401b97093c1f2d345425",
            "value": 801
          }
        },
        "46307a0fb46944d384cf01e3cb083f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64c25245c52245f880adb769f09db2e8",
            "placeholder": "​",
            "style": "IPY_MODEL_3d40e194eb9d478c8206707c668847d1",
            "value": " 801/801 [00:00&lt;00:00, 45.9kB/s]"
          }
        },
        "1c3794e9f73146beade36de2e29d035e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88ae9052decd4afb9726f525be63d80e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0545cca17e2c40eeb4ecb64b33259b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ccada89eec04f66bf4ec0b575da87ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef24f6814844401b97093c1f2d345425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64c25245c52245f880adb769f09db2e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d40e194eb9d478c8206707c668847d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44790a4f605a4de78d39002ee3601559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2b904acd487497ebc107eb524218dcd",
              "IPY_MODEL_865282c93b6e44e2ad43c134111ed608",
              "IPY_MODEL_78fd6ba5b5a34a5ea6a12cf9c90194f1"
            ],
            "layout": "IPY_MODEL_fcadb9a0eb494e3db5f6d70ad788bac5"
          }
        },
        "c2b904acd487497ebc107eb524218dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a850eb1a608463daa541abf7391e0eb",
            "placeholder": "​",
            "style": "IPY_MODEL_a4ea473699ea42a7b1dbd19532b262ba",
            "value": "model.safetensors: 100%"
          }
        },
        "865282c93b6e44e2ad43c134111ed608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c9cb922e76c48988469916420f27f28",
            "max": 2239618772,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a939842b17a54726a1119d0af0a5c7f6",
            "value": 2239618772
          }
        },
        "78fd6ba5b5a34a5ea6a12cf9c90194f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f594c5856104ef78cb3aed1b128c229",
            "placeholder": "​",
            "style": "IPY_MODEL_a70b6e2912f343889362c06a838f50ef",
            "value": " 2.24G/2.24G [00:23&lt;00:00, 136MB/s]"
          }
        },
        "fcadb9a0eb494e3db5f6d70ad788bac5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a850eb1a608463daa541abf7391e0eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4ea473699ea42a7b1dbd19532b262ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c9cb922e76c48988469916420f27f28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a939842b17a54726a1119d0af0a5c7f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f594c5856104ef78cb3aed1b128c229": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a70b6e2912f343889362c06a838f50ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickragv/LLM-DOCUMENT/blob/main/llmparse-complex-documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LlamaParse - Parsing Documents with complex structures"
      ],
      "metadata": {
        "id": "dph56Zw1n8QP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yUXrcALn6s3",
        "outputId": "9c8fbd09-d3d4-4eab-dfec-3ed1a41f952f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU llama-index llama-parse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU llama-index-postprocessor-flag-embedding-reranker git+https://github.com/FlagOpen/FlagEmbedding.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFjIawUYooEY",
        "outputId": "fb0d6353-7dc6-41ce-a2fa-86dbd7e86cb4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/nvidia-cufft-cu12/\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for FlagEmbedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = getpass.getpass(\"LLamaParse API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfpUpTSppzpv",
        "outputId": "5cda190f-3b58-4421-945c-c2bfea567cfc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLamaParse API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtIz_2klrQF_",
        "outputId": "7e599ce7-f11f-42f2-ba81-6b7dd43e4b19"
      },
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from google.colab import files\n",
        "\n",
        "nvidia_earnings_report = files.upload()\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "lhth9gaHox0D",
        "outputId": "5aff60a6-8827-49f8-fe94-d0e07b4e8759"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8b1e10fc-6cf2-4961-9c18-7395bc6c6041\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8b1e10fc-6cf2-4961-9c18-7395bc6c6041\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving financial-results-standalone-q2fy24.pdf to financial-results-standalone-q2fy24.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_parse import LlamaParse\n",
        "\n",
        "parser = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    verbose=True,\n",
        "    language=\"en\",\n",
        "    num_workers=2,\n",
        ")\n",
        "documents = parser.load_data([\"./jp-morgan-financial-report.pdf\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNS39g9npLUU",
        "outputId": "e2b34c9e-8ed3-46c1-9b14-a887c6dad6f1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id a8c24b34-2be7-4f18-bdd2-e3efe04c0592\n",
            "."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(documents[0].text[:2000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibzRkt_Upe1P",
        "outputId": "fe332485-1301-43be-9388-140fd1566eef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## JPMorgan Chase & Co.\n",
            "\n",
            "383 Madison Avenue, New York, NY 10179-0001\n",
            "\n",
            "NYSE symbol: JPM\n",
            "\n",
            "www.jpmorganchase.com\n",
            "\n",
            "### JPMORGAN CHASE REPORTS FOURTH-QUARTER 2023 NET INCOME OF $9.3 BILLION ($3.04 PER SHARE), INCLUDING THE FDIC SPECIAL ASSESSMENT OF $2.9 BILLION ($0.74 DECREASE PER SHARE)\n",
            "\n",
            "FULL-YEAR 2023 NET INCOME OF $49.6 BILLION ($16.23 PER SHARE)\n",
            "\n",
            "### FOURTH-QUARTER 2023 RESULTS\n",
            "\n",
            "|Metrics|4Q23|2023|\n",
            "|---|---|---|\n",
            "|ROE|12%| |\n",
            "|CET1 Capital Ratios|Std. 15.0% | Adv. 15.0%| |\n",
            "|Std. RWA|$1.7T| |\n",
            "|ROTCE|15%| |\n",
            "|Total Loss-Absorbing Capacity|$514B| |\n",
            "|Cash and marketable securities|$1.4T| |\n",
            "|Average loans|$1.3T| |\n",
            "\n",
            "Excluding significant items, 4Q23 net income of $12.1 billion, EPS of $3.97 and ROTCE of 19%\n",
            "\n",
            "Reported revenue of $38.6 billion and managed revenue of $39.9 billion\n",
            "\n",
            "Expense of $24.5 billion; reported overhead ratio of 63%; managed overhead ratio of 61%; expense included the $2.9 billion FDIC special assessment, which increased the overhead ratio by 7%\n",
            "\n",
            "Credit costs of $2.8 billion included $2.2 billion of net charge-offs and a $598 million net reserve build\n",
            "\n",
            "Average loans up 17%, or up 4% excluding First Republic; average deposits flat, or down 3% excluding First Republic\n",
            "\n",
            "Average deposits down 4%; client investment assets up 47%, or up 25% excluding First Republic\n",
            "\n",
            "Average loans up 27%, or up 6% excluding First Republic; Card Services net charge-off rate of 2.79%\n",
            "\n",
            "Debit and credit card sales volume up 7%\n",
            "\n",
            "Active mobile customers up 8%\n",
            "\n",
            "#1 ranking for Global Investment Banking fees with 8.8% wallet share for the year\n",
            "\n",
            "Markets revenue of $5.8 billion, up 2%, with Fixed Income Markets up 8% and Equity Markets down 8%\n",
            "\n",
            "Gross Investment Banking and Markets revenue of $924 million, up 32%\n",
            "\n",
            "Average loans up 19%, or up 3% excluding First Republic; average deposits down 4%\n",
            "\n",
            "Assets under management (AUM) of $3.4 trillion, up 24%\n",
            "\n",
            "Average loans up 6%, or up 1% excluding First Republic; average deposits down 4%\n",
            "\n",
            "### SIGNIFICANT ITEMS IN 4Q23 RESULTS\n",
            "\n",
            "- $2.9 billion FDIC sp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ],
      "metadata": {
        "id": "agQQ32pSqsnT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
        "\n",
        "node_parser = MarkdownElementNodeParser(llm=OpenAI(model=\"gpt-3.5-turbo\"), num_workers=1)"
      ],
      "metadata": {
        "id": "rL1Y1jsNrUu7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = node_parser.get_nodes_from_documents(documents=[documents[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I-frrfOrZ9d",
        "outputId": "8ca859c6-20cf-40b0-fcb7-eb3bbcc18384"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "8it [00:00, 18935.91it/s]\n",
            "\n",
            "\n",
            "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 12%|█▎        | 1/8 [00:04<00:28,  4.03s/it]\u001b[A\u001b[A\n",
            "\n",
            " 25%|██▌       | 2/8 [00:05<00:15,  2.63s/it]\u001b[A\u001b[A\n",
            "\n",
            " 38%|███▊      | 3/8 [00:07<00:11,  2.36s/it]\u001b[A\u001b[A\n",
            "\n",
            " 50%|█████     | 4/8 [00:30<00:41, 10.33s/it]\u001b[A\u001b[A\n",
            "\n",
            " 62%|██████▎   | 5/8 [00:52<00:44, 14.71s/it]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▌  | 6/8 [01:14<00:34, 17.08s/it]\u001b[A\u001b[A\n",
            "\n",
            " 88%|████████▊ | 7/8 [01:36<00:18, 18.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 8/8 [01:57<00:00, 14.71s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_nodes, objects = node_parser.get_nodes_and_objects(nodes)"
      ],
      "metadata": {
        "id": "e_i6JYdprcGn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "recursive_index = VectorStoreIndex(nodes=base_nodes+objects)"
      ],
      "metadata": {
        "id": "gKc-y1cirkD0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
        "\n",
        "reranker = FlagEmbeddingReranker(\n",
        "    top_n=5,\n",
        "    model=\"BAAI/bge-reranker-large\",\n",
        ")\n",
        "\n",
        "recursive_query_engine = recursive_index.as_query_engine(\n",
        "    similarity_top_k=15,\n",
        "    node_postprocessors=[reranker],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ab5165e09c304699a915bb472fe06221",
            "d3bb1a6436074e43bf8e4794a4467e06",
            "b86e49644b4e4a83ba9888ae7e1167c5",
            "b77da89520314dacb367a987cb8e787c",
            "0b01502a56124680b9d87b48fdb6be5d",
            "dd20a93eea124ffa966ac101e18ac190",
            "f48fef06e5244fef98a19728d39efd54",
            "f62e70cee34f4517830d42ec95f9a556",
            "78a42ae63ab048aa9edb9747d4a46cd6",
            "ff204ef009a540429d761ab9e61bb54e",
            "b4ce836844524f08a40f6dadee76466a",
            "ec13f8d45e0946ceb3659fcd1d2d2a66",
            "cc7d00b932614ebda46a017b7651888f",
            "0a55d291544f4187b2abc000f174be8f",
            "f5aac1392a34481d8329df8e9bd19f76",
            "eb2e480eb21146ac96291d75b20511fd",
            "a412920aea434017b209e80680009bf4",
            "4dabb12609964d30b2b96f4605b54414",
            "340bd82069134258a637ae0b5b2d0b5f",
            "4dc5b79b2bf54f268486e4250d442d84",
            "d6ee5c98fb5d40e0904302037535e44c",
            "d504769e561340049cd775a3af4d2667",
            "e6a61371281b49bf80c710ca7b803c1c",
            "c571842e215548d5bfd0a8e2f2776507",
            "bfdd51c8ec24491cae181baadef9bbd9",
            "cac2eba467d7441d89c5cf0044a46211",
            "3c080f39c5fa4d7db40646a9aaa10999",
            "756c968ec1d144b591ea1b5511250908",
            "5e4a0d1ee96840108b59e6cad9d82466",
            "681db4bd52d1489aabe8abbfa7681d50",
            "6c9a98297e3047c1be1701c4fd910623",
            "03de0872343040499353966b411cdbb5",
            "430fafbc4e0f4d18b056aea569447bb1",
            "d8399c2d93c44c50a50c844eb81f9abe",
            "f0c5e6adaa854492bdf31db59fa20345",
            "33471720cf0c41809dbabe045a7f94a9",
            "ed57f1354fb44fa7b9cb2a432058944b",
            "3d9ba94e0dda4285b39277a94a5f3f86",
            "21da9f75ff9a4c23a0fe144d494c9f44",
            "a47d3c18d062486f85af63f339ed8f89",
            "8fb8c060f2ea484fb9d323419c709bfd",
            "acea3d1613234cef9759dcfd7a5f3372",
            "a05d12af715748778883655cab21f63f",
            "7fa6122586414e23af451a639ed75a8b",
            "7fb7e3eb00334944969b35c42ca8fd94",
            "1670cfaffe2440d7b99b7dcb201aa889",
            "941857c8c9ae495ca535a5ba849e3b6d",
            "46307a0fb46944d384cf01e3cb083f64",
            "1c3794e9f73146beade36de2e29d035e",
            "88ae9052decd4afb9726f525be63d80e",
            "0545cca17e2c40eeb4ecb64b33259b45",
            "2ccada89eec04f66bf4ec0b575da87ea",
            "ef24f6814844401b97093c1f2d345425",
            "64c25245c52245f880adb769f09db2e8",
            "3d40e194eb9d478c8206707c668847d1",
            "44790a4f605a4de78d39002ee3601559",
            "c2b904acd487497ebc107eb524218dcd",
            "865282c93b6e44e2ad43c134111ed608",
            "78fd6ba5b5a34a5ea6a12cf9c90194f1",
            "fcadb9a0eb494e3db5f6d70ad788bac5",
            "8a850eb1a608463daa541abf7391e0eb",
            "a4ea473699ea42a7b1dbd19532b262ba",
            "9c9cb922e76c48988469916420f27f28",
            "a939842b17a54726a1119d0af0a5c7f6",
            "7f594c5856104ef78cb3aed1b128c229",
            "a70b6e2912f343889362c06a838f50ef"
          ]
        },
        "id": "QA1RYJn-rnQF",
        "outputId": "0f54ea3d-f5f2-49f8-9e8e-2645ba532701"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-25' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at /usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py:75> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py\", line 76, in wrap_awaitable\n",
            "    return i, await f\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/async_utils.py\", line 106, in worker\n",
            "    return await job\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/node_parser/relational/base_element.py\", line 185, in _get_table_output\n",
            "    response = await query_engine.aquery(summary_query_str)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/base/base_query_engine.py\", line 61, in aquery\n",
            "    query_result = await self._aquery(str_or_query_bundle)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 206, in _aquery\n",
            "    response = await self._response_synthesizer.asynthesize(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/base.py\", line 277, in asynthesize\n",
            "    response_str = await self.aget_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/compact_and_refine.py\", line 23, in aget_response\n",
            "    return await super().aget_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 359, in aget_response\n",
            "    response = await self._agive_response_single(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 477, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 92, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 243, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 206, in acall\n",
            "    chat_response = await self._llm.achat(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/callbacks.py\", line 68, in wrapped_async_llm_chat\n",
            "    f_return_val = await f(_self, messages, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai/base.py\", line 536, in achat\n",
            "    return await achat_fn(messages, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai/base.py\", line 581, in _achat\n",
            "    response = await aclient.chat.completions.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1334, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1738, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1441, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1532, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "  0%|          | 0/8 [21:10<?, ?it/s]\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-26' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at /usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py:75> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py\", line 76, in wrap_awaitable\n",
            "    return i, await f\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/async_utils.py\", line 106, in worker\n",
            "    return await job\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/node_parser/relational/base_element.py\", line 185, in _get_table_output\n",
            "    response = await query_engine.aquery(summary_query_str)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/base/base_query_engine.py\", line 61, in aquery\n",
            "    query_result = await self._aquery(str_or_query_bundle)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 206, in _aquery\n",
            "    response = await self._response_synthesizer.asynthesize(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/base.py\", line 277, in asynthesize\n",
            "    response_str = await self.aget_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/compact_and_refine.py\", line 23, in aget_response\n",
            "    return await super().aget_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 359, in aget_response\n",
            "    response = await self._agive_response_single(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 477, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 92, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 243, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 206, in acall\n",
            "    chat_response = await self._llm.achat(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/callbacks.py\", line 68, in wrapped_async_llm_chat\n",
            "    f_return_val = await f(_self, messages, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai/base.py\", line 536, in achat\n",
            "    return await achat_fn(messages, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai/base.py\", line 581, in _achat\n",
            "    response = await aclient.chat.completions.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1334, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1738, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1441, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1532, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-27' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at /usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py:75> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py\", line 76, in wrap_awaitable\n",
            "    return i, await f\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/async_utils.py\", line 106, in worker\n",
            "    return await job\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/node_parser/relational/base_element.py\", line 185, in _get_table_output\n",
            "    response = await query_engine.aquery(summary_query_str)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/base/base_query_engine.py\", line 61, in aquery\n",
            "    query_result = await self._aquery(str_or_query_bundle)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 206, in _aquery\n",
            "    response = await self._response_synthesizer.asynthesize(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/base.py\", line 277, in asynthesize\n",
            "    response_str = await self.aget_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/compact_and_refine.py\", line 23, in aget_response\n",
            "    return await super().aget_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 359, in aget_response\n",
            "    response = await self._agive_response_single(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 477, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 92, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 243, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 206, in acall\n",
            "    chat_response = await self._llm.achat(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/callbacks.py\", line 68, in wrapped_async_llm_chat\n",
            "    f_return_val = await f(_self, messages, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai/base.py\", line 536, in achat\n",
            "    return await achat_fn(messages, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai/base.py\", line 581, in _achat\n",
            "    response = await aclient.chat.completions.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1334, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1738, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1441, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1532, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-28' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at /usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py:75> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py\", line 76, in wrap_awaitable\n",
            "    return i, await f\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/async_utils.py\", line 106, in worker\n",
            "    return await job\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/node_parser/relational/base_element.py\", line 185, in _get_table_output\n",
            "    response = await query_engine.aquery(summary_query_str)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/base/base_query_engine.py\", line 61, in aquery\n",
            "    query_result = await self._aquery(str_or_query_bundle)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 206, in _aquery\n",
            "    response = await self._response_synthesizer.asynthesize(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/base.py\", line 277, in asynthesize\n",
            "    response_str = await self.aget_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/compact_and_refine.py\", line 23, in aget_response\n",
            "    return await super().aget_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 359, in aget_response\n",
            "    response = await self._agive_response_single(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 477, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 92, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 243, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 206, in acall\n",
            "    chat_response = await self._llm.achat(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/callbacks.py\", line 68, in wrapped_async_llm_chat\n",
            "    f_return_val = await f(_self, messages, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai/base.py\", line 536, in achat\n",
            "    return await achat_fn(messages, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai/base.py\", line 581, in _achat\n",
            "    response = await aclient.chat.completions.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1334, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1738, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1441, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1532, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-29' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at /usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py:75> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py\", line 76, in wrap_awaitable\n",
            "    return i, await f\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/async_utils.py\", line 106, in worker\n",
            "    return await job\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/node_parser/relational/base_element.py\", line 185, in _get_table_output\n",
            "    response = await query_engine.aquery(summary_query_str)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/base/base_query_engine.py\", line 61, in aquery\n",
            "    query_result = await self._aquery(str_or_query_bundle)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 206, in _aquery\n",
            "    response = await self._response_synthesizer.asynthesize(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/base.py\", line 277, in asynthesize\n",
            "    response_str = await self.aget_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/compact_and_refine.py\", line 23, in aget_response\n",
            "    return await super().aget_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 359, in aget_response\n",
            "    response = await self._agive_response_single(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 477, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 92, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 243, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 206, in acall\n",
            "    chat_response = await self._llm.achat(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/callbacks.py\", line 68, in wrapped_async_llm_chat\n",
            "    f_return_val = await f(_self, messages, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai/base.py\", line 536, in achat\n",
            "    return await achat_fn(messages, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai/base.py\", line 581, in _achat\n",
            "    response = await aclient.chat.completions.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1334, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1738, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1441, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1532, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-31' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at /usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py:75> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py\", line 76, in wrap_awaitable\n",
            "    return i, await f\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/async_utils.py\", line 106, in worker\n",
            "    return await job\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/node_parser/relational/base_element.py\", line 185, in _get_table_output\n",
            "    response = await query_engine.aquery(summary_query_str)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/base/base_query_engine.py\", line 61, in aquery\n",
            "    query_result = await self._aquery(str_or_query_bundle)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 206, in _aquery\n",
            "    response = await self._response_synthesizer.asynthesize(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/base.py\", line 277, in asynthesize\n",
            "    response_str = await self.aget_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/compact_and_refine.py\", line 23, in aget_response\n",
            "    return await super().aget_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 359, in aget_response\n",
            "    response = await self._agive_response_single(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 477, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 92, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 243, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 206, in acall\n",
            "    chat_response = await self._llm.achat(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/callbacks.py\", line 68, in wrapped_async_llm_chat\n",
            "    f_return_val = await f(_self, messages, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai/base.py\", line 536, in achat\n",
            "    return await achat_fn(messages, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai/base.py\", line 581, in _achat\n",
            "    response = await aclient.chat.completions.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1334, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1738, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1441, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1532, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-32' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at /usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py:75> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py\", line 76, in wrap_awaitable\n",
            "    return i, await f\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/async_utils.py\", line 106, in worker\n",
            "    return await job\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/node_parser/relational/base_element.py\", line 185, in _get_table_output\n",
            "    response = await query_engine.aquery(summary_query_str)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/base/base_query_engine.py\", line 61, in aquery\n",
            "    query_result = await self._aquery(str_or_query_bundle)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 206, in _aquery\n",
            "    response = await self._response_synthesizer.asynthesize(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/base.py\", line 277, in asynthesize\n",
            "    response_str = await self.aget_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/compact_and_refine.py\", line 23, in aget_response\n",
            "    return await super().aget_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 359, in aget_response\n",
            "    response = await self._agive_response_single(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 477, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 92, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 243, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 206, in acall\n",
            "    chat_response = await self._llm.achat(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/callbacks.py\", line 68, in wrapped_async_llm_chat\n",
            "    f_return_val = await f(_self, messages, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai/base.py\", line 536, in achat\n",
            "    return await achat_fn(messages, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai/base.py\", line 581, in _achat\n",
            "    response = await aclient.chat.completions.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1334, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1738, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1441, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1532, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            " 50%|█████     | 4/8 [10:42<10:42, 160.62s/it]\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-49' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at /usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py:75> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-wz7CNhUR809GkXnl9RILccRi on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py\", line 76, in wrap_awaitable\n",
            "    return i, await f\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/async_utils.py\", line 106, in worker\n",
            "    return await job\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/node_parser/relational/base_element.py\", line 185, in _get_table_output\n",
            "    response = await query_engine.aquery(summary_query_str)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/base/base_query_engine.py\", line 61, in aquery\n",
            "    query_result = await self._aquery(str_or_query_bundle)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 206, in _aquery\n",
            "    response = await self._response_synthesizer.asynthesize(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/base.py\", line 277, in asynthesize\n",
            "    response_str = await self.aget_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/compact_and_refine.py\", line 23, in aget_response\n",
            "    return await super().aget_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
            "    self.span_drop(id=id, err=e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
            "    h.span_drop(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 45, in span_drop\n",
            "    self.prepare_to_drop_span(id, err, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 33, in prepare_to_drop_span\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 359, in aget_response\n",
            "    response = await self._agive_response_single(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 477, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 92, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 243, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 206, in acall\n",
            "    chat_response = await self._llm.achat(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/callbacks.py\", line 68, in wrapped_async_llm_chat\n",
            "    f_return_val = await f(_self, messages, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai/base.py\", line 536, in achat\n",
            "    return await achat_fn(messages, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai/base.py\", line 581, in _achat\n",
            "    response = await aclient.chat.completions.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1334, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1738, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1441, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1517, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1532, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-wz7CNhUR809GkXnl9RILccRi on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab5165e09c304699a915bb472fe06221"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec13f8d45e0946ceb3659fcd1d2d2a66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6a61371281b49bf80c710ca7b803c1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8399c2d93c44c50a50c844eb81f9abe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fb7e3eb00334944969b35c42ca8fd94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44790a4f605a4de78d39002ee3601559"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query =  \"What is the earning per share for jpmorgan chase in 4Q23?\"\n",
        "response = recursive_query_engine.query(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_r6xTRHwR8P",
        "outputId": "122744c5-1cf5-4a94-e15d-7232f373074c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_22_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the earning per share for jpmorgan chase in 4Q23?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_36_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the earning per share for jpmorgan chase in 4Q23?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_46_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the earning per share for jpmorgan chase in 4Q23?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_50_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the earning per share for jpmorgan chase in 4Q23?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_40_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the earning per share for jpmorgan chase in 4Q23?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_28_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the earning per share for jpmorgan chase in 4Q23?\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQQOTftLxAbG",
        "outputId": "79fcd25c-f7e5-4e23-9c47-9e5ea69e3a43"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The earnings per share for JPMorgan Chase in 4Q23 was $3.04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query =  \"What is the net income for commercial banking  in 4Q23?\"\n",
        "response = recursive_query_engine.query(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg8b7pFMxDnx",
        "outputId": "d0cff7d1-49dc-4168-f52b-90ff5e4de06e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_46_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the net income for commercial banking  in 4Q23?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_50_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the net income for commercial banking  in 4Q23?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_36_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the net income for commercial banking  in 4Q23?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_40_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the net income for commercial banking  in 4Q23?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_28_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the net income for commercial banking  in 4Q23?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_22_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the net income for commercial banking  in 4Q23?\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deU9yVpbxtuy",
        "outputId": "d8f44523-75c0-40e7-ff8d-095a62dff446"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The net income for Commercial Banking in 4Q23 is $1.653 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query =  \"What is the noninterest expense for Asset & wealth managment  in 4Q23?\"\n",
        "response = recursive_query_engine.query(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJbqtKnjxywP",
        "outputId": "397f087a-2caa-4905-83d9-ad360e7c3a09"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_46_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the noninterest expense for Asset & wealth managment  in 4Q23?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_50_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the noninterest expense for Asset & wealth managment  in 4Q23?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_36_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the noninterest expense for Asset & wealth managment  in 4Q23?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_40_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the noninterest expense for Asset & wealth managment  in 4Q23?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_22_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the noninterest expense for Asset & wealth managment  in 4Q23?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_28_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the noninterest expense for Asset & wealth managment  in 4Q23?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_0fc4bced-e8d8-4286-a667-13da4c0dd782_6_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the noninterest expense for Asset & wealth managment  in 4Q23?\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1SpHZRUyJc1",
        "outputId": "c918de56-d799-4e27-d796-30edd28b9c95"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The noninterest expense for Asset & Wealth Management in 4Q23 is $3.388 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUB8XUh7yK__"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}